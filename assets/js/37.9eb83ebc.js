(window.webpackJsonp=window.webpackJsonp||[]).push([[37],{435:function(e,t,a){"use strict";a.r(t);var r=a(2),i=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"ppc"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ppc"}},[e._v("#")]),e._v(" PPC")]),e._v(" "),t("p",[e._v("PPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的UNIX网络服务器所采用的模型。基本的流程图是：")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2022/png/375413/1648812284703-9b8d0203-b985-4f05-920d-7f4dcd8bc04d.png#clientId=uae78257e-974b-4&from=paste&height=475&id=u444a8607&name=image.png&originHeight=475&originWidth=505&originalType=binary&ratio=1&rotation=0&showTitle=false&size=55265&status=done&style=none&taskId=ue5cb09bf-0756-4cb7-bd25-42a22f57817&title=&width=505",alt:"image.png"}})]),e._v(" "),t("ul",[t("li",[e._v("父进程接受连接（图中 accept ）")]),e._v(" "),t("li",[e._v("父进程 “fork” 子进程（图中 fork ）")]),e._v(" "),t("li",[e._v("子进程处理连接的读写请求（图中子进程 read、业务处理、write ）")]),e._v(" "),t("li",[e._v("子进程关闭连接（图中子进程中的 close ）")])]),e._v(" "),t("p",[e._v("注意，图中有一个小细节，父进程 “fork” 子进程后，直接调用了 close ，看起来好像是关闭了连接，其实只是将连接的文件描述符引用计数减一，真正的关闭连接是等子进程也调用 close 后，连接对应的文件描述符引用计数变为 0 后，操作系统才会真正关闭连接")]),e._v(" "),t("p",[e._v("PPC 模式实现简单，比较适合服务器的连接数没那么多的情况，例如数据库服务器。对于普通的业务服务器，在互联网兴起之前，由于服务器的访问量和并发量并没有那么大，这种模式其实运作得也挺好，世界上第一个 web服务器 CERN httpd 就采用了这种模式")]),e._v(" "),t("p",[e._v("互联网兴起后，服务器的并发和访问量从几十剧增到成千上万，这种模式的弊端就凸显出来了，主要体现在这几个方面：")]),e._v(" "),t("ol",[t("li",[e._v("fork 代价高：站在操作系统的角度，创建一个进程的代价是很高的，需要分配很多内核资源，需要将内存映像从父进程复制到子进程。即使现在的操作系统在复制内存映像时用到了 Copy on Write（写时复制）技术，总体来说创建进程的代价还是很大的")]),e._v(" "),t("li",[e._v("父子进程通信复杂：父进程 “fork” 子进程时，文件描述符可以通过内存映像复制从父进程传到子进程，但“fork” 完成后，父子进程通信就比较麻烦了，需要采用 IPC（Interprocess Communication）之类的进程通信方案。例如，子进程需要在 close 之前告诉父进程自己处理了多少个请求以支撑父进程进行全局的统计，那么子进程和父进程必须采用 IPC 方案来传递信息。")]),e._v(" "),t("li",[e._v("支持的并发连接数量有限：如果每个连接存活时间比较长，而且新的连接又源源不断的进来，则进程数量会越来越多，操作系统进程调度和切换的频率也越来越高，系统的压力也会越来越大。因此，一般情况下，PPC 方案能处理的并发连接数量最大也就几百")])]),e._v(" "),t("h3",{attrs:{id:"prefork"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prefork"}},[e._v("#")]),e._v(" prefork")]),e._v(" "),t("p",[e._v("PPC 模式中，当连接进来时才 fork 新进程来处理连接请求，由于 fork 进程代价高，用户访问时可能感觉比较慢，prefork 模式的出现就是为了解决这个问题")]),e._v(" "),t("p",[e._v("顾名思义，prefork 就是提前创建进程（pre-fork）。系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去 fork 进程的操作，让用户访问更快、体验更好。prefork 的基本示意图是：\n"),t("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2022/png/375413/1648811681601-9fb75f1d-2b32-42de-a1ef-7d5d772a1178.png#clientId=u660578e2-6ea5-4&from=paste&height=595&id=ub8cad18a&name=image.png&originHeight=1396&originWidth=1411&originalType=binary&ratio=1&rotation=0&showTitle=false&size=173117&status=done&style=none&taskId=u8b434e9b-54a3-4196-9e3b-17bd31d3060&title=&width=601",alt:"image.png"}}),e._v("\nprefork 的实现关键就是多个子进程都 accept 同一个 socket，当有新的连接进入时，操作系统保证只有一个进程能最后 accept 成功。但这里也存在一个小小的问题：“惊群”现象，就是指虽然只有一个子进程能 accept 成功，但所有阻塞在 accept 上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了。幸运的是，操作系统可以解决这个问题，例如 Linux 2.6 版本后内核已经解决了 accept 惊群问题")]),e._v(" "),t("p",[e._v("prefork 模式和 PPC 一样，还是存在父子进程通信复杂、支持的并发连接数量有限的问题，因此目前实际应用也不多。Apache服务器提供了MPM prefork模式，推荐在需要可靠性或者与旧软件兼容的站点时采用这种模式，默认情况下最大支持256个并发连接。")]),e._v(" "),t("h2",{attrs:{id:"tpc"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tpc"}},[e._v("#")]),e._v(" TPC")]),e._v(" "),t("p",[e._v("TPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。与进程相比，线程更轻量级，创建线程的消耗比进程要少得多；同时多线程是共享进程内存空间的，线程通信相比进程通信更简单。因此，TPC 实际上是解决或者弱化了 PPC fork 代价高的问题和父子进程通信复杂的问题。TPC的基本流程是：\n"),t("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2022/png/375413/1648811832652-cb57410f-d1d6-41c2-b289-96512a2fbc04.png#clientId=u69c37846-b65d-4&from=paste&height=395&id=u0b2539c6&name=image.png&originHeight=395&originWidth=497&originalType=binary&ratio=1&rotation=0&showTitle=false&size=52680&status=done&style=none&taskId=u3a480e1f-1d34-4ba0-87b0-54a3df42122&title=&width=497",alt:"image.png"}})]),e._v(" "),t("ul",[t("li",[e._v("父进程接受连接（图中 accept ）")]),e._v(" "),t("li",[e._v("父进程创建子线程（图中 pthread ）")]),e._v(" "),t("li",[e._v("子线程处理连接的读写请求（图中子线程 read、业务处理、write ）")]),e._v(" "),t("li",[e._v("子线程关闭连接（图中子线程中的 close ）")])]),e._v(" "),t("p",[e._v("注意，和 PPC 相比，主进程不用 “close” 连接了。原因是在于子线程是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次 close 即可")]),e._v(" "),t("p",[e._v("TPC 虽然解决了 fork 代价高和进程通信复杂的问题，但是也引入了新的问题，具体表现在：")]),e._v(" "),t("ol",[t("li",[e._v("创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题")]),e._v(" "),t("li",[e._v("无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）"),t("strong",[e._v("(线程共享空间就会有并发问题)")])]),e._v(" "),t("li",[e._v("除了引入了新的问题，TPC 还是存在 CPU 线程调度和切换代价的问题。因此，TPC 方案本质上和 PPC 方案基本类似，在并发几百连接的场景下，反而更多地是采用PPC的方案，因为PPC方案不会有死锁的风险，也不会多进程互相影响，稳定性更高")])]),e._v(" "),t("h3",{attrs:{id:"prethread"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prethread"}},[e._v("#")]),e._v(" prethread")]),e._v(" "),t("p",[e._v("TPC 模式中，当连接进来时才创建新的线程来处理连接请求，虽然创建线程比创建进程要更加轻量级，但还是有一定的代价，而 prethread 模式就是为了解决这个问题。和 prefork 类似，prethread 模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作，让用户感觉更快、体验更好。由于多线程之间数据共享和通信比较方便，因此实际上 prethread 的实现方式相比 prefork 要灵活一些，常见的实现方式有下面几种：")]),e._v(" "),t("ol",[t("li",[e._v("主进程accept，然后将连接交给某个线程处理")]),e._v(" "),t("li",[e._v("子线程都尝试去accept，最终只有一个线程accept成功，方案的基本示意图如下：")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://cdn.nlark.com/yuque/0/2022/png/375413/1648812129224-6fb1351a-ff60-4a8d-82d3-8c17cbac9c54.png#clientId=u69c37846-b65d-4&from=paste&height=571&id=u5fd6fab4&name=image.png&originHeight=1295&originWidth=1411&originalType=binary&ratio=1&rotation=0&showTitle=false&size=175983&status=done&style=none&taskId=u798ef057-fb26-4b96-a51f-2e47095aa45&title=&width=622",alt:"image.png"}})])])}),[],!1,null,null,null);t.default=i.exports}}]);